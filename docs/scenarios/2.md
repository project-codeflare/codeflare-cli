# The CodeFlare Stack - Scenario 2

### Pre-Train a RoBERTa Language Model from Pre-tokenized Data (Using Demo Data)

[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
is a robustly optimized method for pretraining natural language
processing (NLP) systems.

**Goals**: Learning about CodeFlare<br>
**You Provide**: nothing, it just works!<br>
**CodeFlare Stack Provides**: [S3](https://aws.amazon.com/s3/) data **|** [Ray](https://www.ray.io/) cluster **|** [Kubernetes](https://kubernetes.io/) management **|** Distributed training job **|** Pop-up Dashboards

---

To start:

```shell
codeflare ml/codeflare/training/roberta
```

### CLI In Action

You may run the CodeFlare RoBERTa model architecture against sample
data, as we have done in these recordings:

<a href="https://asciinema.org/a/517993" target="_blank"><img src="https://asciinema.org/a/517993.svg" width="600" /></a>

### Pop-up CodeFlare Dashboard In Action

https://user-images.githubusercontent.com/4741620/187531069-12a5dbd3-1b3f-45e8-b8e9-d0940bdc7db1.mp4

[Back to Top](README.md)
